---
title: "SuSiE for Nonparametric Regression Summary"
author: "Kaiqian Zhang"
date: "4/22/2019"
header-includes:
   - \usepackage{dsfont}
output: 
  workflowr::wflow_html:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1 Introduction

We consider a nonparametric regression problem for estimating a regression function $f$, from observations $\{(x_i, y_i): i=1,2,\dots, n\}$, with $x_i$ and $y_i \in \mathbb{R}$, and $y_i = f(x_i) + \epsilon_i$ for $i = 1,2,\dots,n$, where $\epsilon_i$ are independent, mean $0$, and Gaussian distributed random variables. A popular approach for estimating $f$ is to use linear combinations of a pre-specified set of basis functions, e.g., polynomials, splines, and wavelets. Here, we are interested in two types of basis functions: trend filtering ([Kim et al., 2009](https://web.stanford.edu/~boyd/papers/pdf/l1_trend_filter.pdf)) and wavelets ([Daubechies, 1992](https://jqichina.files.wordpress.com/2012/02/ten-lectures-of-waveletsefbc88e5b08fe6b3a2e58d81e8aeb2efbc891.pdf)). The weights, or coefficients, in such a linear combination are usually determined using some form of penalized regression. For this work, we consider using a sparse regression framework [SuSiE](https://stephenslab.github.io/susieR/), which is a fast Bayesian sparse regression method newly developed by [Stephens lab](https://github.com/stephenslab).

In this summary, we present some efforts have been done in applying SuSiE in nonparametric regression problems, specifically using basis functions of trend filtering and wavelets. In Section 2 we present trend filtering-based SuSiE and its application in solving change points problem. In Section 3 we introduce Haar wavelet-based SuSiE as well as decimated and undecimated versions. In Section 4 we further establish a more general Haar wavelet-based SuSiE method, which allows irregularly spaced data and the number to be not a power of $2$. In Section 5 we consider another family of wavelets Symlet and present results from simulation studies related. In Section 6 we discuss one estimating residual variance problem of SuSiE found throughout the work and how we can address this problem by using MAD method.

## 2 Trend filtering-based SuSiE

Nonparametric regression with trend filtering basis is defined mathematically as follows. For a given integer $k \geq 0$, the kth order trend filtering is defined by a penalized least squares optimization problem,
$$
\begin{align}
\hat{\boldsymbol{b}} = \underset{\boldsymbol{b}}{\mathrm{argmin}} \frac{1}{2}|| \boldsymbol{y}- \boldsymbol{b}||_2^2 + \frac{n^k}{k!}\lambda||D_{k+1} \boldsymbol{b}||_1,
\end{align}
$$
where $\boldsymbol{y} = [y_1, y_2, \dots y_n]^T$ is an n vector of observations, $\lambda$ is a tuning parameter, and $D_{k+1}$ is the discrete difference operator of order $k$. When order $k=0$, $D$ is defined
$$
\begin{align}\label{D1}
D_{1} = \begin{bmatrix} 
    -1 & 1 & 0 & \dots & 0 & 0\\
    0 & -1 & 1 & \dots & 0 & 0\\
    \vdots & \ddots & \\
    0 & 0 & 0 & \dots & -1 & 1\\
    \end{bmatrix}
    \in \mathbb{R}^{(n-1)\times n}.
\end{align}
$$
In this case, components of trend filtering estimate form a piecewise constant structure, with break points corresponding to the nonzero entries of $D_{1}\hat{\boldsymbol{b}} = (\hat{b}_2 - \hat{b}_1, \dots, \hat{b}_n - \hat{b}_{n-1})$ ([Tibshirani, 2014](http://www.stat.cmu.edu/~ryantibs/papers/trendfilter.pdf)). And when $k\geq 1$, the operator $D_{k+1}$ is defined recursively,

$$
\begin{align}
D_{k+1} = D_{1} \cdot D_{k} \in \mathbb{R}^{(n-k-1)\times n},
\end{align}
$$
where the dot product is matrix multiplication. Notice that $D_1$ in the equation above is the $(n-k-1)\times (n-k)$ version of $D_1$ matrix as described before.

Now we want to transform the trend filtering problem into a sparse regression problem. Let $\boldsymbol{\beta}=D_{k+1}\boldsymbol{b}$. Then if $D_{k+1}$ were invertibe, we could write $\boldsymbol{b} = (D_{k+1})^{-1}\boldsymbol{\beta}$ and the above problem would become

$$
\begin{align}
\hat{\boldsymbol{\beta}} = \underset{\boldsymbol{\beta}}{\mathrm{argmin}} \frac{1}{2}||\boldsymbol{y}- (D_{k+1})^{-1}\boldsymbol{\beta}||_2^2 + \frac{n^k}{k!}\lambda||\boldsymbol{\beta}||_1.
\end{align}
$$
We can consider this a sparse regression with $\ell_1$ regularization problem, where the design matrix is $(D_{k+1})^{-1}$. We also know that given a deisgn matrix $X$ and a vector of observations $\boldsymbol{y}$, SuSiE performs sparse regression to estimate coefficients $\boldsymbol{\beta}$. Therefore, trend filtering-based SuSiE is just substituting the design matrix $X$ with $(D_{k+1})^{-1}$. Implementation details and tricks associated can be seen from `SuSiE Derivation` (Overleaf) and [`susieR` package](https://github.com/stephenslab/susieR/blob/master/R/trend_filtering_multiplication.R). And notice that we use Haar wavelets in the `susie_trendfilter` implementation. 

We tried `susie_trendfilter` on some [minimal examples](https://stephenslab.github.io/susieR/articles/trend_filtering.html) and discovered that trend filtering-based SuSiE performs well in estimating piecewise constant functions, i.e., change points detection problem, except in one scenario when two change points are in a quick succession. We later used [MAD method](https://kaiqianzhang.github.io/susie-np/susie-tf-MAD-20190411.html), which will be discussed in Section 6, to help soothe the problem. More details of simulation studies associated are [here](https://kaiqianzhang.github.io/susie-np/susie-tf-MAD-20190416.html). [FIXME: maybe new dsc analysis]

## 3 Decimated and undecimated wavelet transform (DWT and UDWT)

### 3.1 Introduction to wavelets and discrete wavelet transform (DWT)

Now we consider wavelets and start with some background of wavelets. Wavelets are a system of orthonormal basis functions for $L^2([0,1])$. The bases are generated by translations and dilations of special functions $\phi(.)$ and $\psi(.)$ called the father and mother wavelet. So for any $j_0\geq 0$, a funcrion $f\in L^2([0,1])$ can be written as 

$$
f(x) = \sum_{k=0}^{2^{j_0}-1}\alpha_{j_0k}\phi_{j_0k}(x) + \sum_{j=j_0}^{\infty}\sum_{k=0}^{2^j-1}\beta_{jk}\psi_{jk}(x)
$$
where 
$$
\phi_{jk}(x) = 2^{j/2}\phi(2^jx-k), \text{ } \psi_{jk}(x) = 2^{j/2}\psi(2^jx-k).
$$
The coefficients $\alpha_{j_0k}$ and $\beta_{jk}$ are called the father and mother wavelets, respectively. The index $j$ is called the resolution level and $j_0$ is the minimum resolution level. Different choices of $\phi$ and $\psi$ generate various wavelet families, e.g., Haar wavelets and Symlet wavelets. 

We often truncate the above expression for some $J$ such that
$$
f(x) = \sum_{k=0}^{2^{j_0}-1}\alpha_{j_0k}\phi_{j_0k}(x) + \sum_{j=j_0}^{J}\sum_{k=0}^{2^j-1}\beta_{jk}\psi_{jk}(x).
$$
We would consider truncated version wavelets hereafter. For any vector $\boldsymbol{f} = [f(1/n), f(2/n), \dots, f(n/n)]^T$ with $n = 2^J$ for some $J$, we can write a linear combination such that 
$$
\boldsymbol{f} = W^T \boldsymbol{d},
$$
where $\boldsymbol{d} = (\alpha_{j_00}, \dots, \alpha_{2^{j_0}-1}, \beta_{j_00}, \beta_{j_01}, \dots, \beta_{J2^{J}-1})^T$ is the vector of wavelet coefficients, and the rows of $W$ are basis functions evaluated at $x_i = i/n$. By orthogonality, 
$$
\boldsymbol{d} = W\boldsymbol{f}.
$$
This transformation from $\boldsymbol{f}$ to its wavelet coefficients via multiplication by $W$ is known as the discrete/decimated wavelet transform (DWT). The DWT can be computed in $O(n)$ operations via Mallat's pyramid algorithm ([Mallat, 1989](https://www.di.ens.fr/~mallat/papiers/MallatTheory89.pdf)). Implementation for this has not been realized in SuSiE yet.

### 3.2 Wavelet-based SuSiE

Given a short introduction of wavelets, we now present wavelet-based SuSiE, which has a similar idea as trend filtering-based SuSiE in Section 2. A nonparametric regression problem in the wavelets context is 
$$
\begin{align}
\hat{\boldsymbol{b}} = \underset{\boldsymbol{b}}{\mathrm{argmin}} \frac{1}{2}|| \boldsymbol{y}- \boldsymbol{b}||_2^2 + \lambda||W \boldsymbol{b}||_1,
\end{align}
$$
where $\boldsymbol{y} = [y_1, y_2, \dots y_n]^T$ is an n vector of observations, $\lambda$ is a tuning parameter, and $W$ is the DWT matrix described before. Let $\boldsymbol{\beta} = W\boldsymbol{b}$. Then $\boldsymbol{b} = W^T\boldsymbol{\beta}$ since $W$ is orthogonal. The above problem similarly becomes 
$$
\begin{align}
\hat{\boldsymbol{\beta}} = \underset{\boldsymbol{\beta}}{\mathrm{argmin}} \frac{1}{2}||\boldsymbol{y}- W^T\boldsymbol{\beta}||_2^2 +\lambda||\boldsymbol{\beta}||_1.
\end{align}
$$
Therefore, wavelet-based SuSiE uses design matrix $W^T$ and observations $\boldsymbol{y}$ to estimate $\boldsymbol{\beta}$.  

### 3.3 Undecimated wavelet transform (UDWT)

We previously introduce the concept of DWT and often use DWT for removing noise from signals and images, which we call denoising. Such denoising usually involves three steps: 

- Use DWT to transform noisy data into an orthogonal domain, i.e., wavelet coefficients.

- Apply soft or hard thresholding to the wavelet coefficients, thereby suppressing those coefficients smaller than a certain amplitude.

- Transform back to the original domain.

Such denoising, however, sometimes exhibits visual artifacts, e.g., [Gibbs phenomena](https://en.wikipedia.org/wiki/Gibbs_phenomenon) in the neighborhood of discontinuity. Coifman and Donoho ([1995](https://web.stanford.edu/dept/statistics/cgi-bin/donoho/wp-content/uploads/2018/08/TIDeNoise.pdf)) attributed this peculiar manner to the lack of translation invariance of the wavelet basis. One method for solving this problem is to use undecimated wavelet transform (UDWT) in the denoising. The intuition is for a range of shifts, one shifts the data, denoises the shifted data, and then unshifts the denoised data. Doing this for each of a range of shifts, and averaging the several results so obtained, produces a reconstruction subject to far weaker Gibbs phenomena than thresholding based denoising using traditional DWT. Cycle-Spinning over the range of all circulant shifts can be accomplished in order n log2(n) time; it is equivalent to denoising using the undecimated or stationary wavelet transform.

One decent denoising method using UDWT is [smashr](https://github.com/stephenslab/smashr) ([Xing and Stephens, 2016](https://arxiv.org/abs/1605.07787)), which will be used to compare with wavelet-based SuSiE later in the simulation studies. We tried minimal examples on wavelet-based SuSiE with UDWT basis but [results](https://kaiqianzhang.github.io/susie-np/wavelet-susie-20190117.html) are not well as expected.

## 4 Wavelet-based SuSiE for irregularly spaced data

Given that we have n observations $\{(x_i, y_i): i=1,2,\dots, n\}$, $W$ in the Section 3.2 is an n by n matrix. One pain for using wavelets here is we require:

- $n$ is a power of 2, i.e., $n=2^J$ for some $J$;

- $x_i$'s have to be equally spaced, i.e., $x_i = \frac{i}{n}$ for $i=1,2,\dots,n$.

Haris et al. ([2018](https://papers.nips.cc/paper/8112-wavelet-regression-and-additive-models-for-irregularly-spaced-data.pdf)) proposed an n by K interpolation matrix $R$ to help solve this issue. The $(i,j)$th entry of $R$ is 

$$
R_{ij}:=
 \begin{cases} 
      1 & j=1, x_i \leq 1/K \\
      (j+1)-Kx_i & j = \lfloor K x_i \rfloor, x_i \in (1/K,1] \\
      Kx_i - (j-1) & j = \lceil Kx_i \rceil, x_i \in (1/K,1] \\
      0 & \text{otherwise},
   \end{cases}
$$
where $K = 2^{\lceil log_2 n \rceil}$, i.e., K is the smallest power of $2$ number that is greater than n. 

Then the sparse regression we want to solve becomes 

$$
\hat{\beta} = \underset{\beta}{\mathrm{argmin}} \frac{1}{2}||y- RW^T\beta||_2^2 + \lambda||\beta||_1,
$$
where $\boldsymbol{y} = [y_1, y_2, \dots y_n]^T$ is an n vector of observations, $\lambda$ is a tuning parameter, $R$ is an n by K matrix, $W$ is a K by K matrix from Haar DWT with K bases, and $\boldsymbol{\beta}$ is a K by 1 column vector. 

[Initial analysis](https://kaiqianzhang.github.io/susie-np/wavelet-susie-20190228.html) for this interpolated wavelet-based SuSiE renders both good and bad fits, which motivate us to modify the interpoation scheme. To obtain a more robust interpolation scheme that is not greatly affected by a huge gap between neighboring $x_i$, we choose K such that
$$
m = \lceil 10\%\text{quantile}(|x_i-x_{i+1}|) \rceil \\
K = 2^{\lceil \text{log}_2m \rceil}.
$$
With a robust interpolation scheme, we obtained [better fits](https://kaiqianzhang.github.io/susie-np/wavelet-susie-20190228-2.html) for most examples, but still observed poor fits due to converging to a local optimum issue preserved in SuSiE. 

## 5 Symlet wavelet-based SuSiE





## 6 Estimate residual variance: MAD method








## References

- Kim, S.-J., Koh, K., Boyd, S. and Gorinevsky, D. (2009). l1 trend filtering. SIAM Rev. 51 339–360.

- Daubechies, I. (1992) Ten Lectures on Wavelets. SIAM, Philadelphia. 

- Tibshirani, R. J. (2014). Adaptive piecewise polynomial estimation via trend filtering.The Annalsof Statistics  42(1), 285–323.

- Haris, A., Simon, N., & Shojaie, A. (2018). Wavelet regression and additive models for irregularly spaced data. NeurIPS.

- G. Mallat, St'ephane. (1989). A Theory of Multiresolution Signal Decomposition: The Wavelet Representation. IEEE Transactions on Pattern Analysis and Machine Intelligence - PAMI. 11. 

- Coifman R.R., Donoho D.L. (1995) Translation-Invariant De-Noising. In: Antoniadis A., Oppenheim G. (eds) Wavelets and Statistics. Lecture Notes in Statistics, vol 103. Springer, New York, NY.

- Xing Z. and Stephens M. (2016). Smoothing via adaptive shrinkage (smash): denoising poisson and heteroskedastic Gaussian signals. arXiv preprint arXiv:1605.07787.







